## Synopsis

Personally, the best way to learn is to build myself a small prototype based on what I have learn fromthe books/theory. The *tinkering* and *debugging* parts really gave me an intuition that is missing had I just do a proof of concept in paper.


[Here are the jupyter notebooks I created](https://github.com/simpleblob/ml_algorithms_stepbystep), with theory and actual codes side-by-side. Hope these will be useful for someone with similar itention.

## Table of Content

1. *linear regression* - the basics of the basics. This notebook illustrates the solution in closed form using matrix in python.
2. *logistic regression* - another bread and butter algorithm. Mostly useful for probability-type predictions (Churn rate, yes/no, certain classifications). Here I used gradient descent as an optimization method.
3. *Neural Network: Perceptron* - the most simple type of NN. 
4. *Neural Network: Multilayer* - a more elaborate type with a hidden layer in the middle. Illustrates backpropagation method with MNIST digit image dataset.

(More will be updated once finished and cleaned up..)
